{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c84589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c60982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(data):\n",
    "    for label, content in data.items():\n",
    "        # Check for which numeric columns have null values\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                # Add a binary column which tells us if the data was missing\n",
    "                data[label+\"_is_missing\"] = pd.isnull(content)\n",
    "                # Fill missing numeric values with median\n",
    "                data[label] = content.fillna(content.median())\n",
    "            # Check for which categorial columns have null values\n",
    "        elif not pd.api.types.is_numeric_dtype(content):\n",
    "            # Add binary column to indicate whether sample had missing value\n",
    "            data[label+\"_is_missing\"] = pd.isnull(content)\n",
    "            # Turn categories into numbers and add +1 (missing values == -1)\n",
    "            data[label] = pd.Categorical(content).codes + 1\n",
    "\n",
    "\n",
    "def string_cols_to_category(data):\n",
    "    # This will turn all of the string values into category values\n",
    "    for label, content in data.items():\n",
    "        if pd.api.types.is_string_dtype(content):\n",
    "            data[label] = content.astype(\"category\").cat.as_ordered()\n",
    "\n",
    "\n",
    "class DataModel(object):\n",
    "    train = ()\n",
    "    valid = ()\n",
    "    test = ()\n",
    "\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    X_valid = None\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "    y_valid = None\n",
    "\n",
    "    _TRAINING_SIZE = 0.7\n",
    "    _VALIDATION_SIZE = 0.15\n",
    "    _TEST_SIZE = 0.15\n",
    "    _X = None\n",
    "    _y = None\n",
    "\n",
    "    def __init__(self, X, y, train_size=0.7, valid_size=0.15, test_size=0.15):\n",
    "        split_size = train_size + valid_size + test_size\n",
    "        if split_size != 1:\n",
    "            msg = \"Combined split sizes must equal 1.\"\n",
    "            raise ValueError(f\"{msg} Current split size={split_size}\")\n",
    "\n",
    "        self._TRAINING_SIZE = train_size\n",
    "        self._VALIDATION_SIZE = valid_size\n",
    "        self._TEST_SIZE = test_size\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "\n",
    "    def split_train_test_validation(self, data):\n",
    "        len_df = len(data)\n",
    "        train_split = round(self._TRAINING_SIZE * len_df)\n",
    "        valid_split = round(train_split + self._VALIDATION_SIZE * len_df)\n",
    "\n",
    "        self.train = (self._X[:train_split],\n",
    "                      self._y[:train_split])\n",
    "\n",
    "        self.valid = (self._X[train_split:valid_split],\n",
    "                      self._y[train_split:valid_split])\n",
    "\n",
    "        self.test = (self._X[valid_split:],\n",
    "                     self._y[valid_split:])\n",
    "\n",
    "        return (self.train, self.valid, self.test)\n",
    "\n",
    "    def split_train_test(self, test_size=None):\n",
    "        if not test_size:\n",
    "            test_size = self._TEST_SIZE\n",
    "\n",
    "        (self.X_train,\n",
    "         self.X_test,\n",
    "         self.y_train,\n",
    "         self.y_test) = train_test_split(\n",
    "            self._X, self._y, test_size=test_size)\n",
    "\n",
    "    def split_train_valid(self, column):\n",
    "        # Split training and validation data into\n",
    "        self.X_train, self.y_train = self._X.drop(\n",
    "            column, axis=1), self._X[column]\n",
    "        self.X_valid, self.y_valid = self._y.drop(\n",
    "            column, axis=1), self._y[column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cf9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor_Image(DataModel):\n",
    "    _IMG_SIZE = 0\n",
    "    _BATCH_SIZE = 0\n",
    "\n",
    "    # Truth_labels (np.ndarray): an array of label strings\n",
    "    truth_labels: np.ndarray = []\n",
    "    trained_model = None\n",
    "\n",
    "    def __init__(self, X, y, train_size=0.7, valid_size=0.15, test_size=0.15,\n",
    "                 img_size=224, batch_size=32):\n",
    "        super().__init__(X, y, train_size, valid_size, test_size)\n",
    "        self._IMG_SIZE = img_size\n",
    "        self._BATCH_SIZE = batch_size\n",
    "\n",
    "    def split_train_test(self, test_size=None):\n",
    "        if not test_size:\n",
    "            test_size = self._TEST_SIZE\n",
    "\n",
    "        (self.X_train,\n",
    "         self.X_valid,\n",
    "         self.y_train,\n",
    "         self.y_valid) = train_test_split(\n",
    "            self._X, self._y, test_size=test_size)\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Takes an image file path and turns the image into a Tensor.\n",
    "        \"\"\"\n",
    "        # Read in an image file\n",
    "        image = tf.io.read_file(image_path)\n",
    "        # Turn our image into numerical Tensor with 3 color channels (RGB)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        # Convert the color channel values from 0-255 to 0-1 values\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        # Resize the image\n",
    "        image = tf.image.resize(image, size=[self._IMG_SIZE, self._IMG_SIZE])\n",
    "\n",
    "        return image\n",
    "\n",
    "    def get_image_label(self, image_path, label):\n",
    "        \"\"\"\n",
    "        Takes an image file path name and the associated label,\n",
    "        process the image and returns a tuple of (image, label)\n",
    "        \"\"\"\n",
    "        image = self.process_image(image_path)\n",
    "        return image, label\n",
    "\n",
    "    def create_data_batches(self, X,\n",
    "                            y=None, valid_data=False, test_data=False):\n",
    "        \"\"\"\n",
    "        Creates batches of data out of image (X) and label (y) pairs.\n",
    "        It shuffles the data if it's training data but doesnt shuffle\n",
    "        if its validation data.\n",
    "        Also accepts test data as input (no labels).\n",
    "        \"\"\"\n",
    "        # If the data is a test data set, we probably don't have labels\n",
    "        if test_data:\n",
    "            data = tf.data.Dataset.from_tensor_slices(\n",
    "                (tf.constant(X)))  # only filepaths\n",
    "            self.test = data.map(self.process_image).batch(self._BATCH_SIZE)\n",
    "        # If the data is a valid dataset, we dont need to shuffle it.\n",
    "        elif valid_data:\n",
    "            data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
    "                                                      tf.constant(y)))\n",
    "            self.valid = data.map(self.get_image_label).batch(self._BATCH_SIZE)\n",
    "        else:\n",
    "            # Turn filepaths and labels into Tensors\n",
    "            data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
    "                                                      tf.constant(y)))\n",
    "\n",
    "            # Shuffle before mapping image processor function is faster\n",
    "            data = data.shuffle(buffer_size=len(X))\n",
    "            data = data.map(self.get_image_label)\n",
    "\n",
    "            # Finally turn the training data into batches\n",
    "            self.train = data.batch(self._BATCH_SIZE)\n",
    "\n",
    "    def create_model(self, model_url: str,\n",
    "                     input_shape: list = None,\n",
    "                     output_shape: list = None,\n",
    "                     activation=\"softmax\", metrics=\"accuracy\"):\n",
    "        \"\"\"Defines the layers in a Keras model in a sequential fashion, then\n",
    "        compiles and builds the model.\n",
    "\n",
    "        Args:\n",
    "            model_url (str): model URL from TensorFlow Hub\n",
    "            input_shape (list, optional): input shape to the model.\n",
    "                Defaults to None.\n",
    "            activation (str, optional): layer density activation.\n",
    "                Defaults to \"softmax\".\n",
    "            metrics (str, optional): metric to be evaluated by the model.\n",
    "                Defaults to \"accuracy\".\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Sequential: the keras model\n",
    "        \"\"\"\n",
    "        if not input_shape:\n",
    "            input_shape = [self._IMG_SIZE, self._IMG_SIZE, 3]\n",
    "\n",
    "        # Setup model layers\n",
    "        model = tf.keras.Sequential([\n",
    "            hub.KerasLayer(model_url),\n",
    "            tf.keras.layers.Dense(units=output_shape,\n",
    "                                  activation=activation)\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=[metrics]\n",
    "        )\n",
    "\n",
    "        # Build the model\n",
    "        model.build(input_shape)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Create a function to build a TensorBoard callback\n",
    "    def create_tensorboard_callback(self, logpath: str,\n",
    "                                    strftime: str = \"%Y%m%d-%H%M%S\"):\n",
    "        \"\"\"Save logs to a directory and pass it to the model's fit() function\n",
    "\n",
    "        Args:\n",
    "            logpath (str): path to store log files\n",
    "            strftime (str, optional): datetime string format.\n",
    "                Defaults to \"%Y%m%d-%H%M%S\".\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.callbacks.TensorBoard: the TensorBoard\n",
    "        \"\"\"\n",
    "        # Create a log directory for storing TensorBoard logs\n",
    "        logdir = Path(\n",
    "            logpath, datetime.datetime.now().strftime(strftime))\n",
    "\n",
    "        return tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "    def train_model(self, input_shape: list,\n",
    "                    output_shape: int,\n",
    "                    model_url: str, epochs: int,\n",
    "                    logpath: str, strftime: str):\n",
    "        \"\"\"Trains a given model and returns the trained version.\n",
    "\n",
    "        Args:\n",
    "            input_shape (list, optional): input shape to the model.\n",
    "                Defaults to None.\n",
    "            model_url (str): model URL from TensorFlow Hub\n",
    "            epochs (int): the number of epochs\n",
    "            logpath (str): path to store log files\n",
    "            strftime (str, optional): datetime string format.\n",
    "                Defaults to \"%Y%m%d-%H%M%S\".\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Sequential: the keras model\n",
    "        \"\"\"\n",
    "        # Create a model\n",
    "        model = self.create_model(input_shape=input_shape,\n",
    "                                  output_shape=output_shape,\n",
    "                                  model_url=model_url)\n",
    "\n",
    "        # Create new TensorBoard session erverytime we train a model\n",
    "        tensorboard = self.create_tensorboard_callback(logpath, strftime)\n",
    "\n",
    "        # Create early stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=3)\n",
    "\n",
    "        # fit the model to the data passing it the callbacks we created\n",
    "        model.fit(x=self.train,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=self.valid,\n",
    "                  validation_freq=1,\n",
    "                  callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "        # Return the fitted model\n",
    "        self.trained_model = model\n",
    "\n",
    "    def get_pred_label(self, prediction_probabilities: np.ndarray):\n",
    "        \"\"\"Turns an array of prediction probabilities into a label\n",
    "\n",
    "        Args:\n",
    "            prediction_probabilities (np.ndarray):\n",
    "                array of prediction probabilities\n",
    "\n",
    "        Returns:\n",
    "            str: the label representation of the prediction\n",
    "        \"\"\"\n",
    "        pred_label = self.truth_labels[np.argmax(prediction_probabilities[0])]\n",
    "        return pred_label\n",
    "\n",
    "    def unbatchify(self, data):\n",
    "        \"\"\"Takes a batched dataset of (image, label) Tensors and returns\n",
    "        separate arrays of images and labels.\n",
    "\n",
    "        Args:\n",
    "            data (BatchDataset): batch dataset to be unbatched\n",
    "\n",
    "        Returns:\n",
    "            set(np.ndarray, np.ndarray): set of arrays (images, labels)\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        # loop through unbatched data\n",
    "        for image, label in data.unbatch().as_numpy_iterator():\n",
    "            images.append(image)\n",
    "            labels.append(self.truth_labels[np.argmax(label)])\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def plot_pred(self, n=1):\n",
    "        \"\"\"View the prediction, ground truth, and image for sample n\n",
    "\n",
    "        Args:\n",
    "            prediction_probabilities (np.ndarray):\n",
    "                array of prediction probabilities\n",
    "            n (int, optional): _description_. index number\n",
    "        \"\"\"\n",
    "\n",
    "        images, labels = self.unbatchify(self.valid)\n",
    "        \n",
    "        predictions = self.trained_model.predict(self.valid, verbose=1)\n",
    "        pred_prob = predictions[n]\n",
    "        true_label = labels[n]\n",
    "        image = images[n]\n",
    "\n",
    "        # Get the pred label\n",
    "        pred_label = self.get_pred_label(pred_prob)\n",
    "\n",
    "        # Plot image a& remove ticks\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        # Change the color of the title depending if the pred is right or wrong\n",
    "        if pred_label == true_label:\n",
    "            color = \"green\"\n",
    "        elif pred_label != true_label:\n",
    "            color = \"red\"\n",
    "\n",
    "        # Change plot title to be predicted, probability of pred, and truth\n",
    "        plt.title(\"{} {:2.0f}% {}\".format(pred_label,\n",
    "                                          np.max(pred_prob)*100,\n",
    "                                          true_label),\n",
    "                  color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e980160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the classes from the common utils for a full blown test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "CWD_PATH = Path.cwd().parent\n",
    "DOG_VISION = CWD_PATH / \"images/Dog Vision\"\n",
    "\n",
    "labels_csv = pd.read_csv(DOG_VISION / \"labels.csv\")\n",
    "labels = labels_csv['breed'].to_numpy()\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Set the number of images\n",
    "NUM_IMAGES = 1000\n",
    "# Set the image size\n",
    "IMG_SIZE = 224\n",
    "# Setup input shape to the model\n",
    "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, color channels\n",
    "# Setup output shape of the model\n",
    "OUTPUT_SHAPE = len(unique_labels)\n",
    "# Setup model URL from TensorFlow Hub\n",
    "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/5\"\n",
    "NUM_EPOCHS = 100\n",
    "logpath = DOG_VISION / \"cb_logs\"\n",
    "strftime = \"%Y%m%d-%H%M%S\"\n",
    "\n",
    "# Setup X & y variables\n",
    "X = [str(DOG_VISION / f\"train/{fname}.jpg\") for fname in labels_csv[\"id\"]]\n",
    "y = [label == unique_labels for label in labels]\n",
    "\n",
    "# create the datamodel of total size NUM_IMAGES\n",
    "data_model = Tensor_Image(X[:NUM_IMAGES], y[:NUM_IMAGES], img_size=IMG_SIZE)\n",
    "\n",
    "data_model.truth_labels = unique_labels\n",
    "\n",
    "# Split them into training and validation \n",
    "data_model.split_train_test(test_size=0.2)\n",
    "\n",
    "# Create training and validation data batches\n",
    "data_model.create_data_batches(data_model.X_train, data_model.y_train)\n",
    "data_model.create_data_batches(data_model.X_valid, data_model.y_valid, valid_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447895e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation data (not used to train on)\n",
    "data_model.train_model(input_shape=INPUT_SHAPE,\n",
    "                       output_shape=OUTPUT_SHAPE,\n",
    "                       model_url=MODEL_URL, \n",
    "                       epochs=NUM_EPOCHS, \n",
    "                       logpath=logpath, strftime=strftime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd253ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.trained_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.plot_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95aed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
