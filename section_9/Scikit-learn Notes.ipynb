{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b09876",
   "metadata": {},
   "source": [
    "# What we're covering in the Scikit-Learn Introduction\n",
    "This notebook outlines the content convered in the Scikit-Learn Introduction.\n",
    "\n",
    "It's a quick stop to see all the Scikit-Learn functions and modules for each section outlined.\n",
    "\n",
    "What we're covering follows the following diagram detailing a Scikit-Learn workflow.\n",
    "\n",
    "<img src=\"../images/sklearn-workflow-title.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdaedb",
   "metadata": {},
   "source": [
    "## 0. Standard library imports\n",
    "For all machine learning projects, you'll often see these libraries (Matplotlib, NumPy and pandas) imported at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c51cb5",
   "metadata": {},
   "source": [
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26a6ef",
   "metadata": {},
   "source": [
    "## 1. Getting our data ready to be used with machine learning\n",
    "Three main things we have to:\n",
    "1. Split the data into features and labels (usually `X` & `y`)\n",
    "2. Filling (also called inputting) or disregarding missing values\n",
    "\n",
    "   __Notes__: The practice of filling missing data is called __imputation__. And it's important to remember there's no perfect way to fill missing data. The techniques you use will depend heavily on your dataset. A good place to look would be searching for \"data imputation techniques\".\n",
    "  \n",
    "   `SimpleImputer()` transforms data by filling missing values with a given strategy. And we can use it to fill the missing values in our DataFrame.\n",
    "  \n",
    "   We split data into train & test to perform filling missing values on them separately.\n",
    "  \n",
    "   We use `fit_transform()` on the training data and `transform()` on the testing data. In essence, we learn the patterns in the training set and transform it via imputation (fit, then transform). Then we take those same patterns and fill the test set (transform only).\n",
    "  \n",
    "3. Converting non-numerical values to numerical values (also called feature encoding)\n",
    "\n",
    "The key takeaways to remember are:\n",
    "\n",
    "* Most datasets you come across won't be in a form ready to immediately start using them with machine learning models. And some may take more preparation than others to get ready to use.\n",
    "\n",
    "* For most machine learning models, your data has to be numerical. This will involve converting whatever you're working with into numbers. This process is often referred to as __feature engineering__ or __feature encoding__.\n",
    "\n",
    "* Some machine learning models aren't compatible with missing data. The process of filling missing data is referred to as __data imputation__.\n",
    "\n",
    "Keep these in mind:\n",
    "* Always keep your training & test data separate\n",
    "* Test sets separately (this goes for filling data with pandas as well)\n",
    "* Don't use data from the future (test set) to fill data from the past (training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f31f8",
   "metadata": {},
   "source": [
    "## 2. Pick a model/estimator (to suit your problem)\n",
    "To pick a model we use the __[Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)__.\n",
    "<img src=\"../images/sklearn-ml-map.png\" />\n",
    "__Notes:__ \n",
    "* sklearn refers to ML models, algorithms as estimators.\n",
    "* Classification problem - predicting a category (heart disease or not)\n",
    "   * Sometimes you'll see `clf` (short for classifier) used as a classification estimator\n",
    "* Regression problem - predicting a number (selling prices of a car)\n",
    "\n",
    "If you're working on a ML problem and looking to use Sklearn and not sure what model you should use, refer to the [Sklearn Machine Learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "What if `Ridge` didn't work or the score didn't fit our needs? Try a different model... How about we try an ensemble model (an ensemble is combination of smaller models to try and make better predictions than just a single model)?\n",
    "\n",
    "Sklearn's ensemble models can be found here: https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "Tidbit:\n",
    "1. If you have structured data, use ensemble methods\n",
    "2. If you have unstructured data, use deep learning or transfer learning (e.g. music, video, images, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee2824",
   "metadata": {},
   "source": [
    "## 3 Fit the model/algorithm on our data and use it to make predictions\n",
    "\n",
    "### Fitting the model to the data\n",
    "\n",
    "Different names for:\n",
    "* `X` = features, feature variables, data\n",
    "* `y` = labels, targets, target variables\n",
    "\n",
    "#### Random Forest model deep dive\n",
    "\n",
    "These resources will help you understand what's happening inside the Random Forest models we've been using.\n",
    "\n",
    "* [Random Forest Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "* [Random Forest Wikipedia (simple version)](https://simple.wikipedia.org/wiki/Random_forest)\n",
    "* [Random Forests in Python](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0) by Will Koehrsen\n",
    "* [An implementation and Explanation of the Random Forest in Python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76) by Will Koehrsen\n",
    "\n",
    "### Make predictions using the machine learning model\n",
    "\n",
    "2 ways to make predictions:\n",
    "\n",
    "1. `predict()`\n",
    "2. `predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07836978",
   "metadata": {},
   "source": [
    "## 4. Evaluating a machine learning model\n",
    "Every Scikit-Learn model has a default metric which is accessible through the `score()` function.\n",
    "\n",
    "However there are a range of different evaluation metrics you can use depending on the model you're using.\n",
    "\n",
    "Three ways to evaluate Scikit-Learn models/estimators:\n",
    "1. Estimator's built in score method\n",
    "2. The `scoring` parameter\n",
    "3. Problem-specific metric functions\n",
    "\n",
    "A full list of evaluation metrics can be __[found in the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)__.\n",
    "\n",
    "### Classification model evaluation metrics\n",
    "\n",
    "1. Accuracy\n",
    "2. Area under ROC curve\n",
    "3. Confusion matrix\n",
    "4. Classification report\n",
    "\n",
    "#### Area under the receiver operating characteristic curve (AUC/RUC)\n",
    "\n",
    "* Area under curve (AUC)\n",
    "* ROC curve\n",
    "\n",
    "ROC curves are a comparison of a model's true positive rate (tpr) versus a models false positive rate (fpr)\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1\n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.\n",
    "\n",
    "In essence, giving you an idea of where the model is getting confused.\n",
    "\n",
    "To summarize classification metrics:\n",
    "* __Accuracy__ is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labeled with 0 or 1)\n",
    "* __Precision__ and __recall__ become more important when classes are imbalanced.\n",
    "* If false positive predictions are worse than false negatives, aim for a higher precision.\n",
    "* If false negative predictions are worse that false positives, aim for a higher recall.\n",
    "* __F1-score__ is a combination of precision and recall.\n",
    "\n",
    "### Regression model evaluation metrics\n",
    "\n",
    "Model evaluation metrics [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "\n",
    "The ones we're going to cover are:\n",
    "1. R^2 (pronounced r-squared) or coefficient of determination\n",
    "2. Mean absolute error (MAE)\n",
    "3. Mean squared error (MSE)\n",
    "\n",
    "#### R^2\n",
    "\n",
    "What R-squared does: Compares your models predictions to the mean of the targets. Values can range from negative infinity ( a very poor model) to 1. For example, if all your model does is predict the mean of the targets, it's R^2 value would be 0. And if your model perfetly predicts a range of nubmers, it's R^2 value would be 1.\n",
    "\n",
    "#### Mean absolute error (MAE)\n",
    "\n",
    "MAE is the average of the absolut differences between predictions and actual values.\n",
    "\n",
    "It gives you an idea of how wrong your models predictions are.\n",
    "\n",
    "#### Mean squared error (MSE)\n",
    "\n",
    "MSE is the mean of the square of the errors between actual and predicted values.\n",
    "\n",
    "Below are some of the most important evaluation metrics you'll want to look into for classification and regression models.\n",
    "\n",
    "### Classification Model Evaluation Metrics/Techniques\n",
    "\n",
    "* __Accuracy__ - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n",
    "\n",
    "* __[Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)__ - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "\n",
    "* __[Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)__ - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "\n",
    "* __[F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)__ - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "\n",
    "* __[Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)__ - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagonal line).\n",
    "\n",
    "* __[Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)__ - Splits your dataset into multiple parts and train and tests your model on each part then evaluates performance as an average.\n",
    "\n",
    "* __[Classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)__ - Sklearn has a built-in function called `classification_report()` which returns some of the main classification metrics such as precision, recall and f1-score.\n",
    "\n",
    "* __[ROC Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_score.html)__ - Also known as [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a plot of true positive rate versus false-positive rate.\n",
    "\n",
    "* __[Area Under Curve (AUC) Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)__ - The area underneath the ROC curve. A perfect model achieves an AUC score of 1.0.\n",
    "\n",
    "### Which classification metric should you use?\n",
    "\n",
    "* __Accuracy__ is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1).\n",
    "\n",
    "* __Precision__ and __recall__ become more important when classes are imbalanced.\n",
    "\n",
    "* If false-positive predictions are worse than false-negatives, aim for higher precision.\n",
    "\n",
    "* If false-negative predictions are worse than false-positives, aim for higher recall.\n",
    "\n",
    "* __F1-score__ is a combination of precision and recall.\n",
    "\n",
    "* A confusion matrix is always a good way to visualize how a classification model is going.\n",
    "\n",
    "### Regression Model Evaluation Metrics/Techniques\n",
    "\n",
    "* [R^2 (pronounced r-squared) or the coefficient of determination](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) - Compares your model's predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1.\n",
    "\n",
    "* [Mean absolute error (MAE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were.\n",
    "\n",
    "* [Mean squared error (MSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors).\n",
    "\n",
    "### Which regression metric should you use?\n",
    "\n",
    "* __R2__ is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your __R2__ value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.\n",
    "\n",
    "* __MAE__ gives a better indication of how far off each of your model's predictions are on average.\n",
    "\n",
    "* As for __MAE__ or __MSE__, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).\n",
    "\n",
    "  * Pay more attention to MAE: When being \\$10,000 off is __twice__ as bad as being \\$5,000 off.\n",
    "\n",
    "  * Pay more attention to MSE: When being \\$10,000 off is __more than twice__ as bad as being \\$5,000 off.\n",
    "\n",
    "For more resources on evaluating a machine learning model, be sure to check out the following resources:\n",
    "\n",
    "* [Scikit-Learn documentation for metrics and scoring (quantifying the quality of predictions)](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "* [Beyond Accuracy: Precision and Recall by Will Koehrsen](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c)\n",
    "\n",
    "* [Stack Overflow answer describing MSE (mean squared error) and RSME (root mean squared error)](https://stackoverflow.com/a/37861832)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b19c7c",
   "metadata": {},
   "source": [
    "#### Data Modelling class to assist with evaluation\n",
    "\n",
    "```Python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DataModel():    \n",
    "    data = []\n",
    "    train = ()\n",
    "    valid = ()\n",
    "    test = ()\n",
    "\n",
    "    _TRAINING_SIZE = 0.7\n",
    "    _VALIDATION_SIZE = 0.15\n",
    "    _TEST_SIZE = 0.15\n",
    "    _X = None\n",
    "    _y = None\n",
    "\n",
    "    def __init__(self, X, y, train_size=0.7, valid_size=0.15, test_size=0.15):\n",
    "        if train_size + valid_size + test_size != 1:\n",
    "            raise ValueError(\n",
    "                f\"Combined split sizes must equal 1. Current split size={train_size + valid_size + test_size}\")\n",
    "\n",
    "        self._TRAINING_SIZE = train_size\n",
    "        self._VALIDATION_SIZE = valid_size\n",
    "        self._TEST_SIZE = test_size\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "\n",
    "    def evaluate_preds(self, y_true, y_preds):\n",
    "        \"\"\"\n",
    "        Performs evaluation comparison on y_true labels vs. y_pred labels on a classification.\n",
    "        \"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_preds)\n",
    "        precision = precision_score(y_true, y_preds)\n",
    "        recall = recall_score(y_true, y_preds)\n",
    "        f1 = f1_score(y_true, y_preds)\n",
    "        return {\"accuracy\": round(accuracy, 2),\n",
    "                \"precision\": round(precision, 2),\n",
    "                \"recall\": round(recall, 2),\n",
    "                \"f1\": round(f1, 2)}\n",
    "\n",
    "    def train_test_validation_split(self, data):\n",
    "        len_df = len(data)\n",
    "        train_split = round(self._TRAINING_SIZE * len_df)\n",
    "        valid_split = round(train_split + self._VALIDATION_SIZE * len_df)\n",
    "\n",
    "        self.train = self._X[:train_split], self._y[:train_split]\n",
    "        self.valid = self._X[train_split:valid_split], self._y[train_split:valid_split]\n",
    "        self.test = self._X[valid_split:], self._y[valid_split:]\n",
    "\n",
    "        return (self.train, self.valid, self.test)\n",
    "\n",
    "    def train_test_split(self, test_size=None):\n",
    "        if not test_size:\n",
    "            test_size = self._TEST_SIZE\n",
    "            \n",
    "        return train_test_split(self._X, self._y, test_size=test_size)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cde36",
   "metadata": {},
   "source": [
    "## 5. Improve through experimentation\n",
    "Two of the main methods to improve a models baseline metrics (the first evaluation metrics you get).\n",
    "\n",
    "From a data perspective ask:\n",
    "\n",
    "* Could we collect more data? In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.\n",
    "* Could we improve our data? This could mean filling in misisng values or finding a better encoding (turning things into numbers) strategy.\n",
    "\n",
    "From a model perspective ask:\n",
    "\n",
    "* Is there a better model we could use? If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the __[Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)__, ensemble methods are generally considered more complex models)\n",
    "* Could we improve the current model? If the model you're using performs well straight out of the box, can the __hyperparameters__ be tuned to make it even better?\n",
    "\n",
    "__Hyperparameters__ are like settings on a model you can adjust so some of the ways it uses to find patterns are altered and potentially improved. Adjusting hyperparameters is referred to as hyperparameter tuning.\n",
    "\n",
    "Hyperparameters vs. Parameters\n",
    "* Parameters = model find these patterns in data\n",
    "* Hyperparameters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters:\n",
    "1. By hand\n",
    "2. Randomly with RandomSearchCV\n",
    "3. Exhaustively with GridSearchCV\n",
    "\n",
    "### 5.1 Fine tuning hyperparameters by hand\n",
    "\n",
    "A very common split used in tuning hyperparameters is by using a 70/15/15 split:\n",
    "\n",
    "* Model gets trained on the Training split\n",
    "* Hyperparameters get tuned on the Validation split\n",
    "* Model gets evaluated on the Test split\n",
    "\n",
    "<img src=\"../images/Data-split-for-training-validation-and-testing.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab02330",
   "metadata": {},
   "source": [
    "## 6. Saving and loading machine learning trained models\n",
    "\n",
    "Two ways to save and load machine learning models:\n",
    "\n",
    "1. With Python's `pickle` module\n",
    "2. With the `joblib` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044cf45",
   "metadata": {},
   "source": [
    "## 7. Putting it all together!\n",
    "\n",
    "Steps we want to do:\n",
    "1. Fill missing data\n",
    "2. Convert data to numbers\n",
    "3. Build a model on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2926da",
   "metadata": {},
   "source": [
    "### Test Case\n",
    "\n",
    "```Python\n",
    "\n",
    "# Getting data ready\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing labels\n",
    "data = pd.read_csv(CAR_SALES_MISSING_CSV)\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipeline\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "door_feature = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"constant\", fill_value=4))\n",
    "])\n",
    "\n",
    "numeric_feature = [\"Odometer (KM)\"]\n",
    "numeric_transformers = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(transformers=[(\"cat\", categorical_transformer, categorical_features), \n",
    "                                               (\"door\", door_transformer, door_feature), \n",
    "                                               (\"num\", numeric_transformers, numeric_feature)\n",
    "                                              ])\n",
    "\n",
    "# Create a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                       (\"model\", RandomForestRegressor())\n",
    "                       ])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "\n",
    "data_model = DataModel(X, y)\n",
    "X_train, X_test, y_train, y_test = data_model.train_test_split(test_size=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test) # score: 0.22188417408787875\n",
    "\n",
    "# Use GridSearchCV with our regression Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"model__n_estimators\": [100, 1000],\n",
    "    \"model__max_depth\": [None, 5],\n",
    "    # \"model__max_features\": [\"auto\"],\n",
    "    \"model__min_samples_split\": [2, 4]\n",
    "}\n",
    "\n",
    "gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)\n",
    "gs_model.fit(X_train, y_train)\n",
    "\n",
    "gs_model.score(X_test, y_test) # Improved score: 0.33876510771726487\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1256468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
